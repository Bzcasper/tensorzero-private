[gateway]
bind_address = "0.0.0.0:3000"
disable_pseudonymous_usage_analytics = true

# ==================================================================
# MODEL DEFINITIONS (Providers & Routing)
# ==================================================================

# --- 1. CEREBRAS (Instant Speed - Best for Drafting) ---
[models.cerebras_llama70b]
routing = ["cerebras"]

[models.cerebras_llama70b.providers.cerebras]
type = "openai"
api_base = "https://api.cerebras.ai/v1"
model_name = "llama3.3-70b" # Latest supported on Cerebras
api_key_location = "env::CEREBRAS_API_KEY"

# --- 2. SAMBANOVA (Massive Reasoning - Best for Judging) ---
[models.sambanova_405b]
routing = ["sambanova"]

[models.sambanova_405b.providers.sambanova]
type = "openai"
api_base = "https://api.sambanova.ai/v1"
model_name = "Meta-Llama-3.1-405B-Instruct"
api_key_location = "env::SAMBANOVA_API_KEY"

# --- 3. GROQ (Reliable Speed) ---
[models.groq_llama33]
routing = ["groq"]

[models.groq_llama33.providers.groq]
type = "groq"
model_name = "llama-3.3-70b-versatile"
api_key_location = "env::GROQ_API_KEY"

# --- 4. MISTRAL (High Quality European Model) ---
[models.mistral_large]
routing = ["mistral"]

[models.mistral_large.providers.mistral]
type = "mistral"
model_name = "mistral-large-latest"
api_key_location = "env::MISTRAL_API_KEY"

# --- 5. DEEPSEEK (Best Value/Logic) ---
[models.deepseek_v3]
routing = ["deepseek"]

[models.deepseek_v3.providers.deepseek]
type = "deepseek"
model_name = "deepseek-chat"
api_key_location = "env::DEEPSEEK_API_KEY"

# --- 6. TOGETHER AI (Qwen 2.5 - SOTA Open Source) ---
[models.qwen_25_together]
routing = ["together"]

[models.qwen_25_together.providers.together]
type = "together"
model_name = "Qwen/Qwen2.5-72B-Instruct-Turbo"
api_key_location = "env::TOGETHER_API_KEY"

# --- 7. PAID MODELS (Gemini & Grok) ---
[models.gemini_pro]
routing = ["google"]
[models.gemini_pro.providers.google]
type = "google_ai_studio_gemini"
model_name = "gemini-1.5-pro"
api_key_location = "env::GEMINI_API_KEY"

[models.grok_2]
routing = ["xai"]
[models.grok_2.providers.xai]
type = "xai"
model_name = "grok-2-1212"
api_key_location = "env::GROK_API_KEY"

# --- 8. OPENROUTER FREE MODELS ---
[models.liquid_lfm_40b]
routing = ["openrouter_liquid"]
[models.liquid_lfm_40b.providers.openrouter_liquid]
type = "openrouter"
model_name = "liquid/lfm-40b"
api_key_location = "env::OPENROUTER_API_KEY"

[models.gemini_2_flash_free]
routing = ["openrouter_gemini_free"]
[models.gemini_2_flash_free.providers.openrouter_gemini_free]
type = "openrouter"
model_name = "google/gemini-2.0-flash-exp:free"
api_key_location = "env::OPENROUTER_API_KEY"


# ==================================================================
# FUNCTION 1: STANDARD CHAT (Load Balanced)
# ==================================================================
[functions.chat]
type = "chat"

# --- Variants ---
[functions.chat.variants.cerebras]
type = "chat_completion"
model = "cerebras_llama70b"
temperature = 0.6
max_tokens = 8192

[functions.chat.variants.sambanova]
type = "chat_completion"
model = "sambanova_405b"
temperature = 0.7
max_tokens = 4096

[functions.chat.variants.groq]
type = "chat_completion"
model = "groq_llama33"
temperature = 0.6
max_tokens = 8192

[functions.chat.variants.mistral]
type = "chat_completion"
model = "mistral_large"
temperature = 0.7
max_tokens = 8192

[functions.chat.variants.deepseek]
type = "chat_completion"
model = "deepseek_v3"
temperature = 0.5
max_tokens = 8192

[functions.chat.variants.gemini_pro]
type = "chat_completion"
model = "gemini_pro"
temperature = 0.5
max_tokens = 8192

[functions.chat.variants.grok]
type = "chat_completion"
model = "grok_2"
temperature = 0.7
max_tokens = 4096

[functions.chat.variants.free_liquid]
type = "chat_completion"
model = "liquid_lfm_40b"
temperature = 0.7
max_tokens = 4096

[functions.chat.variants.free_gemini]
type = "chat_completion"
model = "gemini_2_flash_free"
temperature = 0.7
max_tokens = 4096

# --- Strategy: Weighted Random Routing ---
[functions.chat.experimentation]
type = "static_weights"
[functions.chat.experimentation.candidate_variants]
cerebras = 0.15    # Fast & Free
sambanova = 0.15   # Smart & Free
groq = 0.1         # Fast
mistral = 0.1      # Quality
deepseek = 0.1     # Logic
free_liquid = 0.1  # Free
free_gemini = 0.1  # Free
gemini_pro = 0.1   # Paid (Lower weight)
grok = 0.1         # Paid (Lower weight)


# ==================================================================
# FUNCTION 2: BEST OF N (Quality via Rejection Sampling)
# ==================================================================
# 1. Generate 3 drafts with Cerebras (Instant Llama 3.3).
# 2. Use SambaNova (Llama 405B) to pick the best one.

[functions.best_of_n_chat]
type = "chat"

[functions.best_of_n_chat.variants.generator_cerebras]
type = "chat_completion"
model = "cerebras_llama70b"
temperature = 0.85 # High temp for diverse drafts
top_p = 0.95

[functions.best_of_n_chat.variants.smart_sampler]
type = "experimental_best_of_n_sampling"
candidates = ["generator_cerebras", "generator_cerebras", "generator_cerebras"]
timeout_s = 20

[functions.best_of_n_chat.variants.smart_sampler.evaluator]
model = "sambanova_405b" # 405B model is an excellent judge

[functions.best_of_n_chat.experimentation]
type = "uniform"
candidate_variants = ["smart_sampler"]


# ==================================================================
# FUNCTION 3: MIXTURE OF N (Quality via Fusion)
# ==================================================================
# 1. Generate 3 drafts from diverse high-IQ models (Mistral, DeepSeek, Qwen).
# 2. Use Gemini Pro (Paid, High Context) to fuse them into one answer.

[functions.mixture_of_n_chat]
type = "chat"

# Helper variants for the mixture
[functions.mixture_of_n_chat.variants.draft_mistral]
type = "chat_completion"
model = "mistral_large"
temperature = 0.7

[functions.mixture_of_n_chat.variants.draft_deepseek]
type = "chat_completion"
model = "deepseek_v3"
temperature = 0.6

[functions.mixture_of_n_chat.variants.draft_qwen]
type = "chat_completion"
model = "qwen_25_together"
temperature = 0.7

# The Mixture Strategy
[functions.mixture_of_n_chat.variants.expert_council]
type = "experimental_mixture_of_n"
candidates = ["draft_mistral", "draft_deepseek", "draft_qwen"]
timeout_s = 45

# The Fuser (Gemini Pro integrates the insights)
[functions.mixture_of_n_chat.variants.expert_council.fuser]
model = "gemini_pro"

[functions.mixture_of_n_chat.experimentation]
type = "uniform"
candidate_variants = ["expert_council"]

# ==================================================================
# DIY VIDEO PRODUCTION FUNCTIONS
# ==================================================================

# Fast Inference (Cerebras -> Groq -> OpenRouter)
[models.fast_llm]
routing = ["cerebras", "groq", "openrouter_fast"]

[models.fast_llm.providers.cerebras]
type = "openai"
api_base = "https://api.cerebras.ai/v1"
model_name = "llama3.1-70b"
api_key_location = "env::CEREBRAS_API_KEY"

[models.fast_llm.providers.groq]
type = "groq"
model_name = "llama-3.3-70b-versatile"
api_key_location = "env::GROQ_API_KEY"

[models.fast_llm.providers.openrouter_fast]
type = "openrouter"
model_name = "meta-llama/llama-3.1-70b-instruct"
api_key_location = "env::OPENROUTER_API_KEY"

# Heavy Reasoning (SambaNova -> DeepSeek -> Mistral -> OpenRouter)
[models.smart_llm]
routing = ["sambanova", "deepseek", "mistral", "openrouter_smart"]

[models.smart_llm.providers.sambanova]
type = "openai"
api_base = "https://api.sambanova.ai/v1"
model_name = "Meta-Llama-3.1-405B-Instruct"
api_key_location = "env::SAMBANOVA_API_KEY"

[models.smart_llm.providers.deepseek]
type = "deepseek"
model_name = "deepseek-chat"
api_key_location = "env::DEEPSEEK_API_KEY"

[models.smart_llm.providers.mistral]
type = "mistral"
model_name = "mistral-large-latest"
api_key_location = "env::MISTRAL_API_KEY"

[models.smart_llm.providers.openrouter_smart]
type = "openrouter"
model_name = "openai/gpt-4o"
api_key_location = "env::OPENROUTER_API_KEY"

# Creative/Novelty (Grok -> Gemini -> Together)
[models.creative_llm]
routing = ["xai", "google", "together"]

[models.creative_llm.providers.xai]
type = "xai"
model_name = "grok-2-1212"
api_key_location = "env::GROK_API_KEY"

[models.creative_llm.providers.google]
type = "google_ai_studio_gemini"
model_name = "gemini-1.5-pro"
api_key_location = "env::GEMINI_API_KEY"

[models.creative_llm.providers.together]
type = "together"
model_name = "Qwen/Qwen2.5-72B-Instruct-Turbo"
api_key_location = "env::TOGETHER_API_KEY"

# 1. Script Generator
[functions.script_generator]
type = "json"
description = "Generate structured DIY video script"
system_template = "templates/script_generator_system.minijinja"
user_schema = "templates/script_generator_user_schema.json"
output_schema = "templates/script_output_schema.json"

[functions.script_generator.variants.grok_creative]
type = "chat_completion"
model = "grok_2"
temperature = 0.8
max_tokens = 4096

[functions.script_generator.variants.cerebras_fast]
type = "chat_completion"
model = "cerebras_llama70b"
temperature = 0.7
max_tokens = 4096

# A/B Testing Variants for Script Generation
[functions.script_generator.variants.creative_mode]
type = "chat_completion"
model = "creative_llm"
temperature = 0.9
max_tokens = 4096
json_mode = "strict"

[functions.script_generator.variants.structured_mode]
type = "chat_completion"
model = "smart_llm"
temperature = 0.3
max_tokens = 4096
json_mode = "strict"

# EXPERIMENT 1: Script Logic
# Hypothesis: High-reasoning models (Claude 3.5 Sonnet) create better video pacing than generic models (GPT-4o).
[functions.script_generator.variants.claude_sonnet]
type = "chat_completion"
model = "anthropic::claude-3-5-sonnet-20241022"
temperature = 0.7

[functions.script_generator.variants.gpt4o]
type = "chat_completion"
model = "openai::gpt-4o"
temperature = 0.7

# Neural Router Integration (Modal-based) with track_and_stop as fallback
[functions.script_generator.experimentation]
type = "neural_router"
router_url = "https://bzcasper--video-generation-router-serve-router.modal.run/route"
fallback = "track_and_stop"
context = { function_name = "script_generator" }
timeout_ms = 1000
epsilon = 0.1

# 2. Prompt Enhancer
[functions.prompt_enhancer]
type = "json"
description = "Enhance basic image prompts for better AI generation"
system_template = "templates/prompt_enhancer_system.minijinja"
user_schema = "templates/prompt_enhancer_user_schema.json"
output_schema = "templates/prompt_enhancer_output_schema.json"

[functions.prompt_enhancer.variants.mistral_balanced]
type = "chat_completion"
model = "mistral_large"
temperature = 0.6
max_tokens = 2048

[functions.prompt_enhancer.variants.deepseek_efficient]
type = "chat_completion"
model = "deepseek_v3"
temperature = 0.5
max_tokens = 2048

[functions.prompt_enhancer.variants.cerebras_creative]
type = "chat_completion"
model = "cerebras_llama70b"
temperature = 0.7
max_tokens = 2048

# EXPERIMENT 2: Prompt Enhancement Style
# Hypothesis: "Cinematic" instructions yield better images than "Descriptive" instructions.
[functions.prompt_enhancer.variants.cinematic_style]
type = "chat_completion"
model = "fast_llm" # Llama 3.1 70b
system_template = "templates/prompt_enhancer_system_cinematic.minijinja"

[functions.prompt_enhancer.variants.descriptive_style]
type = "chat_completion"
model = "fast_llm"
system_template = "templates/prompt_enhancer_system_descriptive.minijinja"

[functions.prompt_enhancer.experimentation]
type = "neural_router"
router_url = "https://bzcasper--video-generation-router-serve-router.modal.run/route"
fallback = "static_weights"
context = { function_name = "prompt_enhancer" }
timeout_ms = 1000
epsilon = 0.05

# 3. Video Evaluator (Quality Assurance Judge)
[functions.video_evaluator]
type = "json"
description = "Strict Video Production Quality Assurance AI"
system_template = "templates/video_evaluator_system.minijinja"
user_schema = "templates/video_evaluator_user_schema.json"
output_schema = "templates/video_evaluator_output_schema.json"

[functions.video_evaluator.variants.gemini_judge]
type = "chat_completion"
model = "google_ai_studio_gemini::gemini-1.5-pro" # High reasoning + Vision capable
temperature = 0.1
json_mode = "strict"

[functions.video_evaluator.variants.grok_judge]
type = "chat_completion"
model = "grok_2"
temperature = 0.1
json_mode = "strict"

[functions.video_evaluator.experimentation]
type = "static_weights"
[functions.video_evaluator.experimentation.candidate_variants]
gemini_judge = 0.7
grok_judge = 0.3

# 3. Quality Evaluator
[functions.quality_evaluator]
type = "json"
description = "Evaluate final video quality against requirements"
system_template = "templates/quality_evaluator_system.minijinja"
user_schema = "templates/quality_evaluator_user_schema.json"
output_schema = "templates/quality_evaluator_output_schema.json"

[functions.quality_evaluator.variants.gemini_pro]
type = "chat_completion"
model = "gemini_pro"
temperature = 0.1
max_tokens = 1024
json_mode = "strict"

# ==================================================================
# METRICS FOR QUALITY FEEDBACK & OPTIMIZATION
# ==================================================================

[metrics.video_quality_score]
type = "float"
level = "inference"
optimize = "max"
description = "Overall video quality score (1-10)"

[metrics.production_cost]
type = "float"
level = "episode"
optimize = "min"
description = "Total production cost in USD"

[metrics.human_rating]
type = "boolean"
level = "episode"
optimize = "max"
description = "Human thumbs up/down rating"